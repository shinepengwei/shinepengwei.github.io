---
layout: post
title: 在线学习
category: "理论及算法"
tag: "机器学习"
---
<p>在我们以前介绍的学习模型中，都是批量学习(batch learning)，而我们这一节将会介绍一种新的学习模型——在线学习。在线学习就是样本集成顺序出现，系统一边预测，一边学习。此节内容不属于任何模块，这里简单介绍一下。
</p><p>正式说明如下：有顺序的样例：<img src="/wp-content/uploads/2012/09/092812_1336_1.png" alt=""/>，首先算法根据x&lt;1&gt;预测y&lt;1&gt;，然后系统获得y&lt;1&gt;值，然后根据x&lt;2&gt;…以此类推。。在在线学习中，我们感兴趣的是总错误数量，即<img src="/wp-content/uploads/2012/09/092812_1336_2.png" alt=""/>。
</p><p>拿二值感知器分类举例y={-1,1}，感知器学习算法的假设函数为：
</p><p>????<img src="/wp-content/uploads/2012/09/092812_1336_3.png" alt=""/>
	</p><p>在某一步中，当给出训练样本（x,y）后，感知器学习算法根据规则更新参数。如果预测正确即<img src="/wp-content/uploads/2012/09/092812_1336_4.png" alt=""/>，参数不变，如果预测错误，对参数进行以下更新：
</p><p>????<img src="/wp-content/uploads/2012/09/092812_1336_5.png" alt=""/>
	</p><p>以上在线学习更新算法非常简单，因为感知器学习只需要获知<img src="/wp-content/uploads/2012/09/092812_1336_6.png" alt=""/>的正负号。
</p><p>但是，有定理证明，在线学习的错误数存在上界，并且上界与样本数量m和特征空间维度n没有关系。
</p><p>Video里面并没有对定理的证明进行讲解，本稿也不对其进行证明了，定理说明及证明可参考文稿。
</p>
